{
    "function": "bit_reverse",
    "asm": [
        {
            "type": [
                "int"
            ],
            "instr": [
                "pushq %rbp",
                ".cfi_def_cfa_offset16",
                "pushq %r15",
                ".cfi_def_cfa_offset24",
                "pushq %r14",
                ".cfi_def_cfa_offset32",
                "pushq %r13",
                ".cfi_def_cfa_offset40",
                "pushq %r12",
                ".cfi_def_cfa_offset48",
                "pushq %rbx",
                ".cfi_def_cfa_offset56",
                ".cfi_offset%rbx,-56",
                ".cfi_offset%r12,-48",
                ".cfi_offset%r13,-40",
                ".cfi_offset%r14,-32",
                ".cfi_offset%r15,-24",
                ".cfi_offset%rbp,-16",
                "vandps .LCPI303_0(%rip),%ymm0,%ymm1",
                "vextractf128 $1,%ymm1,%xmm2",
                "vpaddw %xmm1,%xmm1,%xmm1",
                "vpand .LCPI303_1(%rip),%xmm1,%xmm1",
                "vpaddw %xmm2,%xmm2,%xmm2",
                "vpand .LCPI303_1(%rip),%xmm2,%xmm2",
                "vandps .LCPI303_2(%rip),%ymm0,%ymm4",
                "vextractf128 $1,%ymm4,%xmm3",
                "vpextrq $1,%xmm3,%r13",
                "vinsertf128 $1,%xmm2,%ymm1,%ymm2",
                "vpextrq $1,%xmm4,%rsi",
                "vmovq %xmm3,%rdi",
                "vmovq %xmm4,%rbp",
                "vmovd %xmm4,%r10d",
                "movq %rbp,%r11",
                "movq %rdi,%r9",
                "shldq $47,%rsi,%r9",
                "movq %rsi,%rbx",
                "shldq $55,%rbp,%rbx",
                "movq %rsi,%rcx",
                "shldq $47,%rbp,%rcx",
                "shrq %r11",
                "movq %r13,%r8",
                "shrq $17,%r8",
                "vpextrd $1,%xmm4,%r15d",
                "shrl $25,%r10d",
                "shrl %r15d",
                "vmovq %rcx,%xmm0",
                "movq %r13,%r14",
                "shrq $41,%r14",
                "vmovq %rbx,%xmm1",
                "movq %rdi,%r12",
                "vpunpcklbw %xmm0,%xmm1,%xmm0",
                "vmovd %r11d,%xmm1",
                "vpunpcklbw %xmm1,%xmm0,%xmm0",
                "vmovdqa .LCPI303_3(%rip),%xmm8",
                "vpshufb %xmm8,%xmm0,%xmm0",
                "vpinsrb $3,%r10d,%xmm0,%xmm0",
                "shrq $41,%r12",
                "movq %rsi,%rbx",
                "movq %rbp,%rcx",
                "shrq $41,%rcx",
                "vpinsrb $4,%r15d,%xmm0,%xmm0",
                "movq %rdi,%r10",
                "shldq $15,%rsi,%r10",
                "vpinsrb $5,%ecx,%xmm0,%xmm0",
                "movq %rsi,%rcx",
                "shldq $15,%rbp,%rcx",
                "shrdq $57,%rsi,%rbp",
                "shrq $41,%rbx",
                "vmovq %rbp,%xmm1",
                "movq %rdi,%rbp",
                "shldq $7,%rsi,%rbp",
                "vmovq %rcx,%xmm5",
                "vpextrd $2,%xmm4,%ecx",
                "shrl %ecx",
                "vpunpcklbw %xmm1,%xmm5,%xmm1",
                "vpunpcklwd %xmm1,%xmm0,%xmm0",
                "vmovdqa .LCPI303_4(%rip),%xmm9",
                "vpshufb %xmm9,%xmm0,%xmm0",
                "vpinsrb $8,%ecx,%xmm0,%xmm0",
                "movq %rdi,%rdx",
                "shldq $39,%rsi,%rdx",
                "shrl $9,%esi",
                "vpinsrb $9,%esi,%xmm0,%xmm0",
                "movq %r13,%rcx",
                "shrq $49,%rcx",
                "vmovq %rdx,%xmm1",
                "movq %r13,%rsi",
                "vmovq %r9,%xmm5",
                "vpextrd $3,%xmm4,%edx",
                "shrl %edx",
                "vpunpcklbw %xmm1,%xmm5,%xmm1",
                "vpslldq $10,%xmm1,%xmm1",
                "vpblendw $32,%xmm1,%xmm0,%xmm0",
                "vpinsrb $12,%edx,%xmm0,%xmm0",
                "movq %r13,%rdx",
                "shldq $47,%rdi,%rdx",
                "vpinsrb $13,%ebx,%xmm0,%xmm0",
                "movq %r13,%rax",
                "shldq $15,%rdi,%rax",
                "shrq $57,%rsi",
                "vmovq %rbp,%xmm1",
                "vmovd %xmm3,%ebp",
                "shrl %ebp",
                "vpinsrb $0,%ebp,%xmm0,%xmm4",
                "movq %r13,%rbp",
                "shldq $7,%rdi,%rbp",
                "vmovq %r10,%xmm5",
                "movq %r13,%rbx",
                "shldq $39,%rdi,%rbx",
                "shrl $9,%edi",
                "vpinsrb $1,%edi,%xmm4,%xmm4",
                "movq %r13,%rdi",
                "shrq $25,%rdi",
                "vmovq %rbx,%xmm6",
                "vpextrd $1,%xmm3,%ebx",
                "shrl %ebx",
                "vmovq %rdx,%xmm7",
                "vpextrd $2,%xmm3,%edx",
                "vpunpcklbw %xmm6,%xmm7,%xmm6",
                "vpunpcklwd %xmm6,%xmm4,%xmm4",
                "vpinsrb $4,%ebx,%xmm4,%xmm4",
                "vpextrd $3,%xmm3,%ebx",
                "shrl %edx",
                "shrl %ebx",
                "vpinsrb $5,%r12d,%xmm4,%xmm3",
                "vpunpcklbw %xmm1,%xmm5,%xmm1",
                "vmovq %rbp,%xmm4",
                "vmovq %rax,%xmm5",
                "vpunpcklbw %xmm4,%xmm5,%xmm4",
                "vpunpcklwd %xmm4,%xmm3,%xmm3",
                "vpshufb %xmm9,%xmm3,%xmm3",
                "vpinsrb $8,%edx,%xmm3,%xmm3",
                "vpslldq $14,%xmm1,%xmm1",
                "shrl $9,%r13d",
                "vpinsrb $9,%r13d,%xmm3,%xmm3",
                "vpblendw $128,%xmm1,%xmm0,%xmm0",
                "vmovq %rdi,%xmm1",
                "vmovq %r8,%xmm4",
                "vpunpcklbw %xmm1,%xmm4,%xmm1",
                "vpslldq $10,%xmm1,%xmm1",
                "vpblendw $32,%xmm1,%xmm3,%xmm1",
                "vpinsrb $12,%ebx,%xmm1,%xmm1",
                "vpinsrb $13,%r14d,%xmm1,%xmm1",
                "vmovq %rsi,%xmm3",
                "vmovq %rcx,%xmm4",
                "vpunpcklbw %xmm3,%xmm4,%xmm3",
                "vpslldq $14,%xmm3,%xmm3",
                "vpblendw $128,%xmm3,%xmm1,%xmm1",
                "vinsertf128 $1,%xmm1,%ymm0,%ymm0",
                "vorps %ymm0,%ymm2,%ymm0",
                "vandps .LCPI303_5(%rip),%ymm0,%ymm1",
                "vextractf128 $1,%ymm1,%xmm2",
                "vpsllw $2,%xmm1,%xmm1",
                "vpand .LCPI303_6(%rip),%xmm1,%xmm1",
                "vpsllw $2,%xmm2,%xmm2",
                "vpand .LCPI303_6(%rip),%xmm2,%xmm2",
                "vandps .LCPI303_7(%rip),%ymm0,%ymm4",
                "vextractf128 $1,%ymm4,%xmm3",
                "vpextrq $1,%xmm3,%r12",
                "vinsertf128 $1,%xmm2,%ymm1,%ymm2",
                "vpextrq $1,%xmm4,%rbp",
                "vmovq %xmm3,%rax",
                "vmovq %xmm4,%rbx",
                "vmovd %xmm4,%r10d",
                "movq %rbx,%r11",
                "movq %rax,%r9",
                "shldq $46,%rbp,%r9",
                "movq %rbp,%rcx",
                "shldq $54,%rbx,%rcx",
                "movq %rbp,%rsi",
                "shldq $46,%rbx,%rsi",
                "shrq $2,%r11",
                "movq %r12,%r8",
                "shrl $26,%r10d",
                "vpextrd $1,%xmm4,%r14d",
                "shrl $2,%r14d",
                "vmovq %rsi,%xmm0",
                "movq %rax,%r15",
                "shldq $14,%rbp,%r15",
                "vmovq %rcx,%xmm1",
                "movq %rbx,%rcx",
                "vmovd %r11d,%xmm5",
                "movq %rax,%r11",
                "shldq $6,%rbp,%r11",
                "shrq $42,%rcx",
                "vpunpcklbw %xmm0,%xmm1,%xmm0",
                "vpunpcklbw %xmm5,%xmm0,%xmm0",
                "vpshufb %xmm8,%xmm0,%xmm0",
                "vpinsrb $3,%r10d,%xmm0,%xmm0",
                "movq %rbp,%rsi",
                "shldq $14,%rbx,%rsi",
                "shrdq $58,%rbp,%rbx",
                "vpinsrb $4,%r14d,%xmm0,%xmm0",
                "vpextrd $2,%xmm4,%edi",
                "vpinsrb $5,%ecx,%xmm0,%xmm0",
                "shrl $2,%edi",
                "movq %r12,%rcx",
                "shldq $46,%rax,%rcx",
                "vmovq %rbx,%xmm1",
                "movq %rax,%rdx",
                "shldq $38,%rbp,%rdx",
                "vmovq %rsi,%xmm5",
                "vpunpcklbw %xmm1,%xmm5,%xmm1",
                "vpunpcklwd %xmm1,%xmm0,%xmm0",
                "vpshufb %xmm9,%xmm0,%xmm0",
                "vpinsrb $8,%edi,%xmm0,%xmm0",
                "shrq $18,%r8",
                "movq %r12,%r10",
                "movq %rbp,%rdi",
                "shrl $10,%ebp",
                "vpinsrb $9,%ebp,%xmm0,%xmm0",
                "shrq $42,%r10",
                "movq %rax,%r14",
                "vmovq %rdx,%xmm1",
                "vpextrd $3,%xmm4,%edx",
                "shrq $42,%rdi",
                "vmovq %r9,%xmm4",
                "movq %r12,%rbx",
                "shldq $14,%rax,%rbx",
                "shrl $2,%edx",
                "vpunpcklbw %xmm1,%xmm4,%xmm1",
                "vpslldq $10,%xmm1,%xmm1",
                "vpblendw $32,%xmm1,%xmm0,%xmm0",
                "vpinsrb $12,%edx,%xmm0,%xmm0",
                "movq %r12,%rdx",
                "shldq $6,%rax,%rdx",
                "vpinsrb $13,%edi,%xmm0,%xmm0",
                "vmovd %xmm3,%edi",
                "shrl $2,%edi",
                "vpinsrb $0,%edi,%xmm0,%xmm1",
                "movq %r12,%rdi",
                "shldq $38,%rax,%rdi",
                "shrq $42,%r14",
                "shrl $10,%eax",
                "vpinsrb $1,%eax,%xmm1,%xmm1",
                "movq %r12,%rax",
                "shrq $50,%rax",
                "vmovq %rdi,%xmm4",
                "movq %r12,%rdi",
                "shrq $58,%rdi",
                "vmovq %rcx,%xmm5",
                "vpextrd $1,%xmm3,%ecx",
                "shrl $2,%ecx",
                "vpunpcklbw %xmm4,%xmm5,%xmm4",
                "vpunpcklwd %xmm4,%xmm1,%xmm1",
                "vpinsrb $4,%ecx,%xmm1,%xmm1",
                "movq %r12,%rcx",
                "shrq $26,%rcx",
                "vmovq %r11,%xmm4",
                "vpextrd $2,%xmm3,%esi",
                "shrl $2,%esi",
                "vmovq %r15,%xmm5",
                "vpextrd $3,%xmm3,%ebp",
                "shrl $2,%ebp",
                "vpunpcklbw %xmm4,%xmm5,%xmm3",
                "vpslldq $14,%xmm3,%xmm3",
                "vpinsrb $5,%r14d,%xmm1,%xmm1",
                "vpblendw $128,%xmm3,%xmm0,%xmm0",
                "vmovq %rdx,%xmm3",
                "vmovq %rbx,%xmm4",
                "vpunpcklbw %xmm3,%xmm4,%xmm3",
                "vpunpcklwd %xmm3,%xmm1,%xmm1",
                "vpshufb %xmm9,%xmm1,%xmm1",
                "vpinsrb $8,%esi,%xmm1,%xmm1",
                "shrl $10,%r12d",
                "vpinsrb $9,%r12d,%xmm1,%xmm1",
                "vmovq %rcx,%xmm3",
                "vmovq %r8,%xmm4",
                "vpunpcklbw %xmm3,%xmm4,%xmm3",
                "vpslldq $10,%xmm3,%xmm3",
                "vpblendw $32,%xmm3,%xmm1,%xmm1",
                "vpinsrb $12,%ebp,%xmm1,%xmm1",
                "vpinsrb $13,%r10d,%xmm1,%xmm1",
                "vmovq %rdi,%xmm3",
                "vmovq %rax,%xmm4",
                "vpunpcklbw %xmm3,%xmm4,%xmm3",
                "vpslldq $14,%xmm3,%xmm3",
                "vpblendw $128,%xmm3,%xmm1,%xmm1",
                "vinsertf128 $1,%xmm1,%ymm0,%ymm0",
                "vorps %ymm0,%ymm2,%ymm2",
                "vextractf128 $1,%ymm2,%xmm3",
                "vpextrq $1,%xmm3,%rbx",
                "vpextrq $1,%xmm2,%rdx",
                "vmovq %xmm3,%rbp",
                "vmovq %xmm2,%rcx",
                "movl %ecx,%r10d",
                "movq %rbp,%r14",
                "shldq $44,%rdx,%r14",
                "movq %rdx,%rax",
                "shldq $52,%rcx,%rax",
                "movq %rdx,%rsi",
                "shldq $44,%rcx,%rsi",
                "vmovq %rsi,%xmm10",
                "vmovq %rax,%xmm11",
                "movq %rbp,%r11",
                "movq %rcx,%rsi",
                "movq %rbp,%r8",
                "movq %rbp,%r9",
                "movq %rdx,%rax",
                "shldq $12,%rcx,%rax",
                "shrdq $60,%rdx,%rcx",
                "vmovq %rcx,%xmm12",
                "vmovq %rax,%xmm13",
                "movq %rbp,%rcx",
                "movq %rbp,%rdi",
                "shldq $36,%rdx,%rdi",
                "movq %rbx,%rax",
                "shldq $44,%rbp,%rax",
                "vmovq %rdi,%xmm14",
                "vmovq %r14,%xmm15",
                "movq %rbx,%r14",
                "shldq $12,%rbp,%r14",
                "movq %rbx,%rdi",
                "shldq $4,%rbp,%rdi",
                "shrdq $28,%rbx,%rbp",
                "vmovq %rbp,%xmm4",
                "vmovq %rax,%xmm5",
                "shrb $4,%r10b",
                "movzbl %r10b,%eax",
                "vmovd %eax,%xmm7",
                "vmovd %xmm2,%eax",
                "vpunpcklbw %xmm10,%xmm11,%xmm1",
                "vmovdqa .LCPI303_9(%rip),%xmm6",
                "vpand %xmm6,%xmm1,%xmm1",
                "shrl $28,%eax",
                "vpunpcklbw %xmm7,%xmm1,%xmm1",
                "vpshufb %xmm8,%xmm1,%xmm1",
                "vpinsrb $3,%eax,%xmm1,%xmm1",
                "vpextrb $4,%xmm2,%eax",
                "shrb $4,%al",
                "movzbl %al,%eax",
                "vpinsrb $4,%eax,%xmm1,%xmm1",
                "vmovq %rdi,%xmm10",
                "shrq $44,%rsi",
                "andb $15,%sil",
                "vpunpcklbw %xmm12,%xmm13,%xmm0",
                "vpand %xmm6,%xmm0,%xmm0",
                "movzbl %sil,%eax",
                "vpinsrb $5,%eax,%xmm1,%xmm1",
                "vpextrb $8,%xmm2,%eax",
                "shrb $4,%al",
                "vpunpcklwd %xmm0,%xmm1,%xmm0",
                "vpshufb %xmm9,%xmm0,%xmm0",
                "movzbl %al,%eax",
                "vpinsrb $8,%eax,%xmm0,%xmm0",
                "movq %rdx,%rax",
                "shldq $12,%rdx,%r8",
                "shldq $4,%rdx,%r9",
                "shrq $12,%rdx",
                "andb $15,%dl",
                "vpunpcklbw %xmm14,%xmm15,%xmm1",
                "movzbl %dl,%edx",
                "vpinsrb $9,%edx,%xmm0,%xmm0",
                "vpextrb $12,%xmm2,%edx",
                "vpand %xmm6,%xmm1,%xmm1",
                "shrb $4,%dl",
                "vpslldq $10,%xmm1,%xmm1",
                "vpblendw $32,%xmm1,%xmm0,%xmm0",
                "movzbl %dl,%edx",
                "vpinsrb $12,%edx,%xmm0,%xmm0",
                "shrq $44,%rax",
                "andb $15,%al",
                "movzbl %al,%eax",
                "vpinsrb $13,%eax,%xmm0,%xmm8",
                "vmovq %r14,%xmm1",
                "vmovd %xmm3,%eax",
                "shrb $4,%al",
                "movzbl %al,%eax",
                "vpinsrb $0,%eax,%xmm0,%xmm0",
                "shrq $12,%rcx",
                "andb $15,%cl",
                "movzbl %cl,%eax",
                "vpinsrb $1,%eax,%xmm0,%xmm0",
                "movq %rbx,%rax",
                "shrq $28,%rax",
                "vmovq %rax,%xmm7",
                "movq %rbx,%rax",
                "shrq $20,%rax",
                "vpunpcklbw %xmm4,%xmm5,%xmm4",
                "vpand %xmm6,%xmm4,%xmm4",
                "vmovq %rax,%xmm5",
                "vpextrb $4,%xmm3,%eax",
                "shrb $4,%al",
                "vpunpcklwd %xmm4,%xmm0,%xmm0",
                "movzbl %al,%eax",
                "vpinsrb $4,%eax,%xmm0,%xmm0",
                "shrq $44,%r11",
                "andb $15,%r11b",
                "vpunpcklbw %xmm10,%xmm1,%xmm1",
                "movzbl %r11b,%eax",
                "vpinsrb $5,%eax,%xmm0,%xmm0",
                "vpextrb $8,%xmm3,%eax",
                "vpand %xmm6,%xmm1,%xmm1",
                "shrb $4,%al",
                "vpunpcklwd %xmm1,%xmm0,%xmm0",
                "vpshufb %xmm9,%xmm0,%xmm0",
                "movzbl %al,%eax",
                "vpinsrb $8,%eax,%xmm0,%xmm0",
                "movq %rbx,%rax",
                "shrq $12,%rax",
                "andb $15,%al",
                "vpunpcklbw %xmm7,%xmm5,%xmm1",
                "vpand %xmm6,%xmm1,%xmm1",
                "movzbl %al,%eax",
                "vpinsrb $9,%eax,%xmm0,%xmm0",
                "vpextrb $12,%xmm3,%eax",
                "shrb $4,%al",
                "vpslldq $10,%xmm1,%xmm1",
                "vpblendw $32,%xmm1,%xmm0,%xmm0",
                "movzbl %al,%eax",
                "vpinsrb $12,%eax,%xmm0,%xmm0",
                "movq %rbx,%rax",
                "shrq $44,%rax",
                "andb $15,%al",
                "movzbl %al,%eax",
                "vpinsrb $13,%eax,%xmm0,%xmm0",
                "movq %rbx,%rax",
                "shrq $52,%rax",
                "andb $15,%al",
                "movzbl %al,%eax",
                "vpinsrb $14,%eax,%xmm0,%xmm0",
                "shrq $60,%rbx",
                "vpinsrb $15,%ebx,%xmm0,%xmm0",
                "vmovq %r9,%xmm1",
                "vpsllw $4,%xmm2,%xmm2",
                "vpand .LCPI303_8(%rip),%xmm2,%xmm2",
                "vmovq %r8,%xmm4",
                "vpsllw $4,%xmm3,%xmm3",
                "vpand .LCPI303_8(%rip),%xmm3,%xmm3",
                "vinsertf128 $1,%xmm3,%ymm2,%ymm2",
                "vpunpcklbw %xmm1,%xmm4,%xmm1",
                "vpand %xmm6,%xmm1,%xmm1",
                "vpslldq $14,%xmm1,%xmm1",
                "vpblendw $128,%xmm1,%xmm8,%xmm1",
                "vinsertf128 $1,%xmm0,%ymm1,%ymm0",
                "vorps %ymm2,%ymm0,%ymm0",
                "vextractf128 $1,%ymm0,%xmm1",
                "vmovdqa .LCPI303_10(%rip),%xmm2",
                "vpshufb %xmm2,%xmm1,%xmm1",
                "vpshufb %xmm2,%xmm0,%xmm0",
                "vinsertf128 $1,%xmm1,%ymm0,%ymm0",
                "popq %rbx",
                ".cfi_def_cfa_offset48",
                "popq %r12",
                ".cfi_def_cfa_offset40",
                "popq %r13",
                ".cfi_def_cfa_offset32",
                "popq %r14",
                ".cfi_def_cfa_offset24",
                "popq %r15",
                ".cfi_def_cfa_offset16",
                "popq %rbp",
                ".cfi_def_cfa_offset8"
            ]
        }
    ]
}