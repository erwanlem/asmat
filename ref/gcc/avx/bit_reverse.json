{
    "function": "bit_reverse",
    "asm": [
        {
            "type": [
                "int"
            ],
            "instr": [
                "pushq %rbp",
                ".cfi_def_cfa_offset16",
                ".cfi_offset6,-16",
                "movq %rsp,%rbp",
                ".cfi_def_cfa_register6",
                "pushq %r15",
                "pushq %r14",
                "pushq %r13",
                "pushq %r12",
                "pushq %rbx",
                "andq $-32,%rsp",
                ".cfi_offset15,-24",
                ".cfi_offset14,-32",
                ".cfi_offset13,-40",
                ".cfi_offset12,-48",
                ".cfi_offset3,-56",
                "vandps .LC35(%rip),%ymm0,%ymm1",
                "vandps .LC36(%rip),%ymm0,%ymm0",
                "vmovdqa .LC37(%rip),%xmm2",
                "vpextrb $6,%xmm1,%edx",
                "vmovdqa %xmm0,%xmm3",
                "vextractf128 $0x1,%ymm0,%xmm0",
                "vpsllw $1,%xmm3,%xmm4",
                "sarl %edx",
                "vpsllw $1,%xmm0,%xmm6",
                "vpand %xmm2,%xmm3,%xmm5",
                "movl %edx,-4(%rsp)",
                "vpextrb $7,%xmm1,%edx",
                "vpand %xmm2,%xmm0,%xmm0",
                "vmovdqa .LC34(%rip),%xmm3",
                "sarl %edx",
                "vpsllw $1,%xmm5,%xmm5",
                "vpextrb $0,%xmm1,%eax",
                "movl %edx,-8(%rsp)",
                "vpextrb $8,%xmm1,%edx",
                "vpand %xmm2,%xmm5,%xmm5",
                "sarl %eax",
                "sarl %edx",
                "vpand %xmm3,%xmm4,%xmm4",
                "vpextrb $2,%xmm1,%r9d",
                "movl %edx,-12(%rsp)",
                "vpsllw $1,%xmm0,%xmm0",
                "vpaddw %xmm5,%xmm4,%xmm4",
                "sarl %r9d",
                "vpextrb $9,%xmm1,%edx",
                "vpand %xmm3,%xmm6,%xmm5",
                "vpand %xmm2,%xmm0,%xmm0",
                "vpaddw %xmm0,%xmm5,%xmm0",
                "vpextrb $4,%xmm1,%esi",
                "sarl %edx",
                "vmovd %r9d,%xmm10",
                "movl %edx,-16(%rsp)",
                "vpextrb $10,%xmm1,%edx",
                "sarl %esi",
                "vinsertf128 $0x1,%xmm0,%ymm4,%ymm4",
                "sarl %edx",
                "vextractf128 $0x1,%ymm1,%xmm0",
                "vmovd %esi,%xmm6",
                "movl %edx,-20(%rsp)",
                "vpextrb $11,%xmm1,%edx",
                "vpextrb $1,%xmm1,%r15d",
                "vpextrb $3,%xmm1,%r14d",
                "sarl %edx",
                "vpextrb $5,%xmm1,%r13d",
                "sarl %r15d",
                "vmovd -4(%rsp),%xmm9",
                "movl %edx,-24(%rsp)",
                "sarl %r14d",
                "vpextrb $12,%xmm1,%edx",
                "sarl %r13d",
                "sarl %edx",
                "vpinsrb $1,-8(%rsp),%xmm9,%xmm9",
                "vpextrb $9,%xmm0,%r12d",
                "movl %edx,-28(%rsp)",
                "vpextrb $13,%xmm1,%edx",
                "sarl %r12d",
                "vpextrb $10,%xmm0,%r8d",
                "sarl %edx",
                "vpextrb $11,%xmm0,%ebx",
                "sarl %r8d",
                "vpextrb $12,%xmm0,%ecx",
                "movl %edx,-32(%rsp)",
                "vpextrb $14,%xmm1,%edx",
                "sarl %ecx",
                "vpextrb $13,%xmm0,%r11d",
                "sarl %edx",
                "vpextrb $14,%xmm0,%edi",
                "vpextrb $15,%xmm0,%r10d",
                "sarl %ebx",
                "movl %edx,-36(%rsp)",
                "sarl %edi",
                "vpextrb $15,%xmm1,%edx",
                "sarl %r11d",
                "sarl %edx",
                "vpinsrb $1,%r14d,%xmm10,%xmm10",
                "vpinsrb $1,%r13d,%xmm6,%xmm6",
                "movl %edx,-40(%rsp)",
                "vpextrb $0,%xmm0,%edx",
                "vpunpcklwd %xmm9,%xmm6,%xmm6",
                "sarl %edx",
                "movl %edx,-44(%rsp)",
                "vpextrb $1,%xmm0,%edx",
                "sarl %edx",
                "movl %edx,-48(%rsp)",
                "vpextrb $2,%xmm0,%edx",
                "sarl %edx",
                "movl %edx,-52(%rsp)",
                "vpextrb $3,%xmm0,%edx",
                "sarl %edx",
                "movl %edx,-56(%rsp)",
                "vpextrb $4,%xmm0,%edx",
                "sarl %edx",
                "movl %edx,-60(%rsp)",
                "vpextrb $5,%xmm0,%edx",
                "sarl %edx",
                "movl %edx,-64(%rsp)",
                "vpextrb $6,%xmm0,%edx",
                "sarl %edx",
                "movl %edx,-68(%rsp)",
                "vpextrb $7,%xmm0,%edx",
                "sarl %edx",
                "movl %edx,-72(%rsp)",
                "vmovd -12(%rsp),%xmm1",
                "vpextrb $8,%xmm0,%edx",
                "vpinsrb $1,-16(%rsp),%xmm1,%xmm1",
                "vmovd -20(%rsp),%xmm8",
                "vmovd -28(%rsp),%xmm5",
                "vmovd %eax,%xmm0",
                "sarl %edx",
                "vmovd -36(%rsp),%xmm7",
                "vpinsrb $1,%r15d,%xmm0,%xmm0",
                "sarl %r10d",
                "vpinsrb $1,-24(%rsp),%xmm8,%xmm8",
                "vpinsrb $1,-40(%rsp),%xmm7,%xmm7",
                "vpunpcklwd %xmm10,%xmm0,%xmm0",
                "vpinsrb $1,-32(%rsp),%xmm5,%xmm5",
                "vpunpcklwd %xmm8,%xmm1,%xmm1",
                "vpunpckldq %xmm6,%xmm0,%xmm0",
                "vmovd -52(%rsp),%xmm10",
                "vpunpcklwd %xmm7,%xmm5,%xmm5",
                "vpinsrb $1,-56(%rsp),%xmm10,%xmm10",
                "vmovd -60(%rsp),%xmm9",
                "vpunpckldq %xmm5,%xmm1,%xmm1",
                "vpinsrb $1,-64(%rsp),%xmm9,%xmm11",
                "vmovd %edx,%xmm6",
                "vmovd -68(%rsp),%xmm9",
                "vpunpcklqdq %xmm1,%xmm0,%xmm0",
                "vpinsrb $1,-72(%rsp),%xmm9,%xmm9",
                "vmovd -44(%rsp),%xmm1",
                "vmovd %r8d,%xmm8",
                "vmovd %ecx,%xmm5",
                "vpinsrb $1,-48(%rsp),%xmm1,%xmm1",
                "vmovd %edi,%xmm7",
                "vpunpcklwd %xmm9,%xmm11,%xmm9",
                "vpinsrb $1,%ebx,%xmm8,%xmm8",
                "vpinsrb $1,%r10d,%xmm7,%xmm7",
                "vpinsrb $1,%r12d,%xmm6,%xmm6",
                "vpinsrb $1,%r11d,%xmm5,%xmm5",
                "vpunpcklwd %xmm8,%xmm6,%xmm6",
                "vpunpcklwd %xmm10,%xmm1,%xmm1",
                "vpunpcklwd %xmm7,%xmm5,%xmm5",
                "vpunpckldq %xmm9,%xmm1,%xmm1",
                "vpunpckldq %xmm5,%xmm6,%xmm5",
                "vpunpcklqdq %xmm5,%xmm1,%xmm1",
                "vinsertf128 $0x1,%xmm1,%ymm0,%ymm0",
                "vorps %ymm0,%ymm4,%ymm1",
                "vandps .LC38(%rip),%ymm1,%ymm0",
                "vandps .LC39(%rip),%ymm1,%ymm1",
                "vpextrb $6,%xmm0,%edx",
                "vpextrb $0,%xmm0,%eax",
                "vmovdqa %xmm1,%xmm4",
                "sarl $2,%edx",
                "vpextrb $1,%xmm0,%r15d",
                "sarl $2,%eax",
                "vpextrb $2,%xmm0,%r9d",
                "movl %edx,-4(%rsp)",
                "vpextrb $7,%xmm0,%edx",
                "sarl $2,%r9d",
                "vpextrb $3,%xmm0,%r14d",
                "sarl $2,%edx",
                "vextractf128 $0x1,%ymm1,%xmm1",
                "sarl $2,%r15d",
                "vmovd %r9d,%xmm10",
                "movl %edx,-8(%rsp)",
                "vpextrb $8,%xmm0,%edx",
                "vpextrb $4,%xmm0,%esi",
                "vpextrb $5,%xmm0,%r13d",
                "sarl $2,%edx",
                "vpsllw $2,%xmm4,%xmm6",
                "vpand %xmm2,%xmm4,%xmm4",
                "sarl $2,%esi",
                "movl %edx,-12(%rsp)",
                "vpand %xmm3,%xmm6,%xmm6",
                "sarl $2,%r14d",
                "vmovd %esi,%xmm9",
                "vpextrb $9,%xmm0,%edx",
                "vpsllw $2,%xmm1,%xmm5",
                "vpand %xmm2,%xmm1,%xmm1",
                "sarl $2,%r13d",
                "sarl $2,%edx",
                "vpsllw $2,%xmm4,%xmm4",
                "vpand %xmm3,%xmm5,%xmm5",
                "movl %edx,-16(%rsp)",
                "vpextrb $10,%xmm0,%edx",
                "vpand %xmm2,%xmm4,%xmm4",
                "vpinsrb $1,%r14d,%xmm10,%xmm10",
                "sarl $2,%edx",
                "vpsllw $2,%xmm1,%xmm1",
                "vpaddw %xmm4,%xmm6,%xmm4",
                "movl %edx,-20(%rsp)",
                "vpextrb $11,%xmm0,%edx",
                "vpand %xmm2,%xmm1,%xmm1",
                "vpinsrb $1,%r13d,%xmm9,%xmm9",
                "sarl $2,%edx",
                "vpaddw %xmm1,%xmm5,%xmm1",
                "movl %edx,-24(%rsp)",
                "vpextrb $12,%xmm0,%edx",
                "vinsertf128 $0x1,%xmm1,%ymm4,%ymm4",
                "vmovd %eax,%xmm1",
                "sarl $2,%edx",
                "vpinsrb $1,%r15d,%xmm1,%xmm1",
                "movl %edx,-28(%rsp)",
                "vpextrb $13,%xmm0,%edx",
                "vpunpcklwd %xmm10,%xmm1,%xmm1",
                "sarl $2,%edx",
                "movl %edx,-32(%rsp)",
                "vpextrb $14,%xmm0,%edx",
                "sarl $2,%edx",
                "movl %edx,-36(%rsp)",
                "vpextrb $15,%xmm0,%edx",
                "vextractf128 $0x1,%ymm0,%xmm0",
                "sarl $2,%edx",
                "vpextrb $9,%xmm0,%r12d",
                "vpextrb $10,%xmm0,%r8d",
                "movl %edx,-40(%rsp)",
                "vpextrb $0,%xmm0,%edx",
                "sarl $2,%r8d",
                "vpextrb $11,%xmm0,%ebx",
                "sarl $2,%edx",
                "vpextrb $12,%xmm0,%ecx",
                "sarl $2,%r12d",
                "vpextrb $13,%xmm0,%r11d",
                "movl %edx,-44(%rsp)",
                "sarl $2,%ecx",
                "vpextrb $1,%xmm0,%edx",
                "sarl $2,%ebx",
                "sarl $2,%edx",
                "vpextrb $14,%xmm0,%edi",
                "vpextrb $15,%xmm0,%r10d",
                "sarl $2,%r11d",
                "movl %edx,-48(%rsp)",
                "vpextrb $2,%xmm0,%edx",
                "sarl $2,%edi",
                "sarl $2,%edx",
                "movl %edx,-52(%rsp)",
                "vpextrb $3,%xmm0,%edx",
                "sarl $2,%edx",
                "movl %edx,-56(%rsp)",
                "vpextrb $4,%xmm0,%edx",
                "sarl $2,%edx",
                "movl %edx,-60(%rsp)",
                "vpextrb $5,%xmm0,%edx",
                "vmovd -4(%rsp),%xmm8",
                "vmovd -20(%rsp),%xmm7",
                "sarl $2,%edx",
                "vpinsrb $1,-8(%rsp),%xmm8,%xmm8",
                "vpinsrb $1,-24(%rsp),%xmm7,%xmm7",
                "movl %edx,-64(%rsp)",
                "vpextrb $6,%xmm0,%edx",
                "vmovd -28(%rsp),%xmm5",
                "vmovd -36(%rsp),%xmm6",
                "sarl $2,%edx",
                "vpinsrb $1,-40(%rsp),%xmm6,%xmm6",
                "vpunpcklwd %xmm8,%xmm9,%xmm9",
                "movl %edx,-68(%rsp)",
                "vpextrb $7,%xmm0,%edx",
                "vpunpckldq %xmm9,%xmm1,%xmm1",
                "vpinsrb $1,-32(%rsp),%xmm5,%xmm5",
                "sarl $2,%edx",
                "vmovd -60(%rsp),%xmm9",
                "vmovd -52(%rsp),%xmm10",
                "movl %edx,-72(%rsp)",
                "vpextrb $8,%xmm0,%edx",
                "vpunpcklwd %xmm6,%xmm5,%xmm5",
                "vmovd -12(%rsp),%xmm0",
                "vpinsrb $1,-16(%rsp),%xmm0,%xmm0",
                "sarl $2,%edx",
                "vmovd %r8d,%xmm8",
                "vpinsrb $1,-64(%rsp),%xmm9,%xmm11",
                "sarl $2,%r10d",
                "vmovd -68(%rsp),%xmm9",
                "vmovd %edx,%xmm6",
                "vpunpcklwd %xmm7,%xmm0,%xmm0",
                "vmovd %edi,%xmm7",
                "vpinsrb $1,-56(%rsp),%xmm10,%xmm10",
                "vpunpckldq %xmm5,%xmm0,%xmm0",
                "vmovd %ecx,%xmm5",
                "vpinsrb $1,-72(%rsp),%xmm9,%xmm9",
                "vpunpcklqdq %xmm0,%xmm1,%xmm1",
                "vmovd -44(%rsp),%xmm0",
                "vpinsrb $1,-48(%rsp),%xmm0,%xmm0",
                "vpinsrb $1,%ebx,%xmm8,%xmm8",
                "vpinsrb $1,%r10d,%xmm7,%xmm7",
                "vpinsrb $1,%r12d,%xmm6,%xmm6",
                "vpinsrb $1,%r11d,%xmm5,%xmm5",
                "vpunpcklwd %xmm8,%xmm6,%xmm6",
                "vpunpcklwd %xmm9,%xmm11,%xmm9",
                "vpunpcklwd %xmm7,%xmm5,%xmm5",
                "vpunpcklwd %xmm10,%xmm0,%xmm0",
                "vpunpckldq %xmm5,%xmm6,%xmm5",
                "vpunpckldq %xmm9,%xmm0,%xmm0",
                "vpunpcklqdq %xmm5,%xmm0,%xmm0",
                "vinsertf128 $0x1,%xmm0,%ymm1,%ymm1",
                "vorps %ymm1,%ymm4,%ymm1",
                "vpextrb $6,%xmm1,%edx",
                "vextractf128 $0x1,%ymm1,%xmm4",
                "vpand %xmm2,%xmm1,%xmm5",
                "sarl $4,%edx",
                "vpsllw $4,%xmm1,%xmm0",
                "vpextrb $0,%xmm1,%eax",
                "movl %edx,-4(%rsp)",
                "vpextrb $7,%xmm1,%edx",
                "vpand %xmm3,%xmm0,%xmm0",
                "vpextrb $1,%xmm1,%r15d",
                "sarl $4,%edx",
                "vpextrb $2,%xmm1,%r9d",
                "sarl $4,%eax",
                "vpextrb $3,%xmm1,%r14d",
                "movl %edx,-8(%rsp)",
                "vpextrb $8,%xmm1,%edx",
                "vpextrb $4,%xmm1,%esi",
                "vpextrb $5,%xmm1,%r13d",
                "sarl $4,%edx",
                "vpsllw $4,%xmm4,%xmm6",
                "vpand %xmm2,%xmm4,%xmm4",
                "sarl $4,%r9d",
                "movl %edx,-12(%rsp)",
                "vpsllw $4,%xmm5,%xmm5",
                "vpand %xmm3,%xmm6,%xmm3",
                "sarl $4,%esi",
                "sarl $4,%r15d",
                "vpand %xmm2,%xmm5,%xmm5",
                "sarl $4,%r14d",
                "vmovd %r9d,%xmm8",
                "vpextrb $9,%xmm1,%edx",
                "vpsllw $4,%xmm4,%xmm4",
                "vpaddw %xmm5,%xmm0,%xmm0",
                "sarl $4,%r13d",
                "sarl $4,%edx",
                "vpand %xmm2,%xmm4,%xmm4",
                "vpinsrb $1,%r14d,%xmm8,%xmm8",
                "movl %edx,-16(%rsp)",
                "vpextrb $10,%xmm1,%edx",
                "vpaddw %xmm4,%xmm3,%xmm2",
                "vmovd %esi,%xmm4",
                "sarl $4,%edx",
                "vinsertf128 $0x1,%xmm2,%ymm0,%ymm0",
                "vpinsrb $1,%r13d,%xmm4,%xmm4",
                "movl %edx,-20(%rsp)",
                "vpextrb $11,%xmm1,%edx",
                "sarl $4,%edx",
                "movl %edx,-24(%rsp)",
                "vpextrb $12,%xmm1,%edx",
                "sarl $4,%edx",
                "movl %edx,-28(%rsp)",
                "vpextrb $13,%xmm1,%edx",
                "sarl $4,%edx",
                "movl %edx,-32(%rsp)",
                "vpextrb $14,%xmm1,%edx",
                "sarl $4,%edx",
                "movl %edx,-36(%rsp)",
                "vpextrb $15,%xmm1,%edx",
                "vextractf128 $0x1,%ymm1,%xmm1",
                "sarl $4,%edx",
                "vpextrb $9,%xmm1,%r12d",
                "vpextrb $10,%xmm1,%r8d",
                "movl %edx,-40(%rsp)",
                "vpextrb $0,%xmm1,%edx",
                "sarl $4,%r8d",
                "vpextrb $11,%xmm1,%ebx",
                "sarl $4,%edx",
                "vpextrb $12,%xmm1,%ecx",
                "sarl $4,%r12d",
                "vpextrb $13,%xmm1,%r11d",
                "movl %edx,-44(%rsp)",
                "sarl $4,%ecx",
                "vpextrb $1,%xmm1,%edx",
                "sarl $4,%ebx",
                "sarl $4,%edx",
                "vpextrb $14,%xmm1,%edi",
                "vpextrb $15,%xmm1,%r10d",
                "sarl $4,%r11d",
                "movl %edx,-48(%rsp)",
                "vpextrb $2,%xmm1,%edx",
                "vmovd -4(%rsp),%xmm7",
                "vmovd -12(%rsp),%xmm2",
                "sarl $4,%edx",
                "vmovd -20(%rsp),%xmm6",
                "sarl $4,%edi",
                "vpinsrb $1,-8(%rsp),%xmm7,%xmm7",
                "movl %edx,-52(%rsp)",
                "vpextrb $3,%xmm1,%edx",
                "vpinsrb $1,-24(%rsp),%xmm6,%xmm6",
                "vmovd -28(%rsp),%xmm3",
                "sarl $4,%edx",
                "vpinsrb $1,-16(%rsp),%xmm2,%xmm2",
                "vpunpcklwd %xmm7,%xmm4,%xmm4",
                "movl %edx,-56(%rsp)",
                "vpextrb $4,%xmm1,%edx",
                "vpinsrb $1,-32(%rsp),%xmm3,%xmm3",
                "vmovd %r8d,%xmm7",
                "sarl $4,%edx",
                "vmovd -36(%rsp),%xmm5",
                "vpunpcklwd %xmm6,%xmm2,%xmm2",
                "movl %edx,-60(%rsp)",
                "vpextrb $5,%xmm1,%edx",
                "vpinsrb $1,-40(%rsp),%xmm5,%xmm5",
                "vmovd -52(%rsp),%xmm9",
                "sarl $4,%edx",
                "vmovd %edi,%xmm6",
                "vpinsrb $1,-56(%rsp),%xmm9,%xmm9",
                "movl %edx,-64(%rsp)",
                "vpextrb $6,%xmm1,%edx",
                "vpunpcklwd %xmm5,%xmm3,%xmm3",
                "vpinsrb $1,%ebx,%xmm7,%xmm7",
                "sarl $4,%edx",
                "vpunpckldq %xmm3,%xmm2,%xmm2",
                "vmovd -60(%rsp),%xmm5",
                "movl %edx,-68(%rsp)",
                "vpextrb $7,%xmm1,%edx",
                "vpinsrb $1,-64(%rsp),%xmm5,%xmm5",
                "vmovd -44(%rsp),%xmm3",
                "sarl $4,%edx",
                "vpinsrb $1,-48(%rsp),%xmm3,%xmm3",
                "movl %edx,-72(%rsp)",
                "vpextrb $8,%xmm1,%edx",
                "vmovd %eax,%xmm1",
                "vpinsrb $1,%r15d,%xmm1,%xmm1",
                "sarl $4,%edx",
                "vpunpcklwd %xmm9,%xmm3,%xmm3",
                "vpunpcklwd %xmm8,%xmm1,%xmm1",
                "sarl $4,%r10d",
                "vmovd -68(%rsp),%xmm8",
                "vpinsrb $1,-72(%rsp),%xmm8,%xmm8",
                "vpunpckldq %xmm4,%xmm1,%xmm1",
                "vmovd %ecx,%xmm4",
                "vpinsrb $1,%r10d,%xmm6,%xmm6",
                "vpunpcklqdq %xmm2,%xmm1,%xmm1",
                "vmovd %edx,%xmm2",
                "vpinsrb $1,%r11d,%xmm4,%xmm4",
                "vpinsrb $1,%r12d,%xmm2,%xmm2",
                "vpunpcklwd %xmm8,%xmm5,%xmm5",
                "vpunpcklwd %xmm6,%xmm4,%xmm4",
                "vpunpcklwd %xmm7,%xmm2,%xmm2",
                "vpunpckldq %xmm5,%xmm3,%xmm3",
                "vpunpckldq %xmm4,%xmm2,%xmm2",
                "vpunpcklqdq %xmm2,%xmm3,%xmm2",
                "vinsertf128 $0x1,%xmm2,%ymm1,%ymm1",
                "vorps %ymm1,%ymm0,%ymm0",
                "vpextrb $9,%xmm0,%ecx",
                "vpextrb $8,%xmm0,%edi",
                "vpextrb $2,%xmm0,%r15d",
                "movl %ecx,-4(%rsp)",
                "vpextrb $15,%xmm0,%ecx",
                "vpextrb $0,%xmm0,%r14d",
                "vpextrb $6,%xmm0,%r13d",
                "movl %edi,-8(%rsp)",
                "vpextrb $14,%xmm0,%edi",
                "vpextrb $4,%xmm0,%r12d",
                "vpextrb $10,%xmm0,%ebx",
                "movl %ecx,-12(%rsp)",
                "vpextrb $3,%xmm0,%eax",
                "vpextrb $13,%xmm0,%ecx",
                "vpextrb $1,%xmm0,%r9d",
                "movl %edi,-16(%rsp)",
                "vpextrb $7,%xmm0,%esi",
                "vmovd %r9d,%xmm7",
                "vpextrb $12,%xmm0,%edi",
                "vpextrb $5,%xmm0,%r8d",
                "vpextrb $11,%xmm0,%edx",
                "movl %ecx,-20(%rsp)",
                "vmovd %esi,%xmm3",
                "vextractf128 $0x1,%ymm0,%xmm0",
                "movl %edi,-24(%rsp)",
                "vmovd %r8d,%xmm6",
                "vmovd %edx,%xmm1",
                "vpextrb $3,%xmm0,%ecx",
                "vpextrb $2,%xmm0,%edi",
                "vpextrb $14,%xmm0,%r11d",
                "movl %ecx,-28(%rsp)",
                "vpextrb $1,%xmm0,%ecx",
                "vpextrb $12,%xmm0,%r10d",
                "vpinsrb $1,%r14d,%xmm7,%xmm7",
                "movl %edi,-32(%rsp)",
                "vpextrb $0,%xmm0,%edi",
                "vpinsrb $1,%r12d,%xmm6,%xmm6",
                "vpinsrb $1,%r13d,%xmm3,%xmm3",
                "movl %ecx,-36(%rsp)",
                "vpextrb $7,%xmm0,%ecx",
                "vmovd -4(%rsp),%xmm5",
                "vmovd -12(%rsp),%xmm2",
                "movl %edi,-40(%rsp)",
                "vpextrb $6,%xmm0,%edi",
                "vpunpcklwd %xmm6,%xmm3,%xmm3",
                "vpinsrb $1,-8(%rsp),%xmm5,%xmm5",
                "movl %ecx,-44(%rsp)",
                "vpextrb $5,%xmm0,%ecx",
                "vpinsrb $1,-16(%rsp),%xmm2,%xmm2",
                "vmovd -20(%rsp),%xmm4",
                "movl %edi,-48(%rsp)",
                "vpextrb $4,%xmm0,%edi",
                "vpinsrb $1,-24(%rsp),%xmm4,%xmm4",
                "vpinsrb $1,%ebx,%xmm1,%xmm1",
                "movl %ecx,-52(%rsp)",
                "vpextrb $11,%xmm0,%ecx",
                "vpunpcklwd %xmm5,%xmm1,%xmm1",
                "vmovd -36(%rsp),%xmm8",
                "movl %edi,-56(%rsp)",
                "vpextrb $10,%xmm0,%edi",
                "vpunpcklwd %xmm4,%xmm2,%xmm2",
                "vpinsrb $1,-40(%rsp),%xmm8,%xmm8",
                "movl %ecx,-60(%rsp)",
                "vpextrb $9,%xmm0,%ecx",
                "vpunpckldq %xmm2,%xmm1,%xmm1",
                "vmovd -44(%rsp),%xmm4",
                "movl %edi,-64(%rsp)",
                "vpextrb $8,%xmm0,%edi",
                "vpinsrb $1,-48(%rsp),%xmm4,%xmm4",
                "movl %ecx,-68(%rsp)",
                "vpextrb $15,%xmm0,%ecx",
                "movl %edi,-72(%rsp)",
                "vpextrb $13,%xmm0,%edi",
                "vmovd %eax,%xmm0",
                "vmovd -60(%rsp),%xmm2",
                "vpinsrb $1,%r15d,%xmm0,%xmm0",
                "vpinsrb $1,-64(%rsp),%xmm2,%xmm2",
                "vmovd %edi,%xmm5",
                "vpunpcklwd %xmm7,%xmm0,%xmm0",
                "vmovd -68(%rsp),%xmm6",
                "vmovd -52(%rsp),%xmm7",
                "vpunpckldq %xmm3,%xmm0,%xmm0",
                "vmovd %ecx,%xmm3",
                "vpinsrb $1,-56(%rsp),%xmm7,%xmm7",
                "vpunpcklqdq %xmm1,%xmm0,%xmm0",
                "vpinsrb $1,-72(%rsp),%xmm6,%xmm6",
                "vmovd -28(%rsp),%xmm1",
                "vpinsrb $1,%r11d,%xmm3,%xmm3",
                "vpinsrb $1,-32(%rsp),%xmm1,%xmm1",
                "vpinsrb $1,%r10d,%xmm5,%xmm5",
                "vpunpcklwd %xmm7,%xmm4,%xmm4",
                "vpunpcklwd %xmm6,%xmm2,%xmm2",
                "vpunpcklwd %xmm5,%xmm3,%xmm3",
                "vpunpcklwd %xmm8,%xmm1,%xmm1",
                "vpunpckldq %xmm3,%xmm2,%xmm2",
                "leaq -40(%rbp),%rsp",
                "vpunpckldq %xmm4,%xmm1,%xmm1",
                "popq %rbx",
                "popq %r12",
                "vpunpcklqdq %xmm2,%xmm1,%xmm1",
                "popq %r13",
                "popq %r14",
                "vinsertf128 $0x1,%xmm1,%ymm0,%ymm0",
                "popq %r15",
                "popq %rbp",
                ".cfi_def_cfa7,8"
            ]
        }
    ]
}