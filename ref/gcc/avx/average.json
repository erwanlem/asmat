{
    "function": "average",
    "asm": [
        {
            "type": [
                "float",
                "float"
            ],
            "instr": [
                "vmovaps .LC10(%rip),%ymm2",
                "vmulps %ymm2,%ymm1,%ymm1",
                "vmulps %ymm2,%ymm0,%ymm0",
                "vaddps %ymm0,%ymm1,%ymm0"
            ]
        },
        {
            "type": [
                "double",
                "double"
            ],
            "instr": [
                "vmovapd .LC11(%rip),%ymm2",
                "vmulpd %ymm2,%ymm1,%ymm1",
                "vmulpd %ymm2,%ymm0,%ymm0",
                "vaddpd %ymm0,%ymm1,%ymm0"
            ]
        },
        {
            "type": [
                "std::int64_t",
                "std::int64_t"
            ],
            "instr": [
                "vxorps %ymm1,%ymm0,%ymm2",
                "vandps %ymm1,%ymm0,%ymm0",
                "vextractf128 $0x1,%ymm2,%xmm1",
                "vmovq %xmm2,%rax",
                "vpextrq $1,%xmm2,%rcx",
                "sarq %rax",
                "vmovq %xmm1,%rdx",
                "sarq %rcx",
                "vmovdqa %xmm0,%xmm3",
                "sarq %rdx",
                "vmovq %rax,%xmm4",
                "vextractf128 $0x1,%ymm0,%xmm0",
                "vmovq %rdx,%xmm2",
                "vpextrq $1,%xmm1,%rdx",
                "vpinsrq $1,%rcx,%xmm4,%xmm1",
                "sarq %rdx",
                "vpinsrq $1,%rdx,%xmm2,%xmm2",
                "vinsertf128 $0x1,%xmm2,%ymm1,%ymm1",
                "vmovdqa %xmm1,%xmm2",
                "vextractf128 $0x1,%ymm1,%xmm1",
                "vpaddq %xmm2,%xmm3,%xmm2",
                "vpaddq %xmm1,%xmm0,%xmm0",
                "vinsertf128 $0x1,%xmm0,%ymm2,%ymm0"
            ]
        },
        {
            "type": [
                "std::int32_t",
                "std::int32_t"
            ],
            "instr": [
                "vxorps %ymm1,%ymm0,%ymm2",
                "vmovdqa %ymm1,%ymm6",
                "vmovd %xmm2,%eax",
                "vpextrd $1,%xmm2,%esi",
                "vpextrd $3,%xmm2,%edx",
                "sarl %eax",
                "sarl %esi",
                "vandps %ymm6,%ymm0,%ymm0",
                "movl %eax,%ecx",
                "vpextrd $2,%xmm2,%eax",
                "vextractf128 $0x1,%ymm2,%xmm2",
                "sarl %edx",
                "vpextrd $1,%xmm2,%edi",
                "sarl %eax",
                "vmovd %ecx,%xmm7",
                "vmovdqa %xmm0,%xmm3",
                "vmovd %eax,%xmm5",
                "vmovd %xmm2,%eax",
                "sarl %edi",
                "vextractf128 $0x1,%ymm0,%xmm0",
                "sarl %eax",
                "vpinsrd $1,%edx,%xmm5,%xmm5",
                "vmovd %eax,%xmm1",
                "vpextrd $2,%xmm2,%eax",
                "sarl %eax",
                "vpinsrd $1,%edi,%xmm1,%xmm1",
                "vmovd %eax,%xmm4",
                "vpextrd $3,%xmm2,%eax",
                "vpinsrd $1,%esi,%xmm7,%xmm2",
                "sarl %eax",
                "vpunpcklqdq %xmm5,%xmm2,%xmm2",
                "vpinsrd $1,%eax,%xmm4,%xmm4",
                "vpaddd %xmm2,%xmm3,%xmm2",
                "vpunpcklqdq %xmm4,%xmm1,%xmm1",
                "vpaddd %xmm1,%xmm0,%xmm0",
                "vinsertf128 $0x1,%xmm0,%ymm2,%ymm0"
            ]
        },
        {
            "type": [
                "std::int16_t",
                "std::int16_t"
            ],
            "instr": [
                "pushq %rbp",
                ".cfi_def_cfa_offset16",
                ".cfi_offset6,-16",
                "vxorps %ymm1,%ymm0,%ymm3",
                "vandps %ymm1,%ymm0,%ymm0",
                "vextractf128 $0x1,%ymm3,%xmm1",
                "vpextrw $0,%xmm3,%eax",
                "vpextrw $2,%xmm3,%r9d",
                "vpextrw $5,%xmm1,%r10d",
                "vpextrw $4,%xmm3,%esi",
                "vpextrw $6,%xmm3,%r8d",
                "cwtl",
                "movq %rsp,%rbp",
                ".cfi_def_cfa_register6",
                "pushq %r15",
                "movswl %r10w,%r10d",
                ".cfi_offset15,-24",
                "vpextrw $1,%xmm3,%r15d",
                "pushq %r14",
                "sarl %r10d",
                ".cfi_offset14,-32",
                "vpextrw $3,%xmm3,%r14d",
                "movswl %r9w,%r9d",
                "pushq %r13",
                "movswl %si,%esi",
                ".cfi_offset13,-40",
                "vpextrw $5,%xmm3,%r13d",
                "movswl %r8w,%r8d",
                "pushq %r12",
                ".cfi_offset12,-48",
                "vpextrw $7,%xmm3,%r12d",
                "sarl %eax",
                "movswl %r15w,%r15d",
                "pushq %rbx",
                "sarl %r9d",
                "movswl %r14w,%r14d",
                "sarl %esi",
                "movswl %r13w,%r13d",
                "sarl %r8d",
                "movswl %r12w,%r12d",
                "vmovdqa %xmm0,%xmm5",
                "sarl %r15d",
                "vextractf128 $0x1,%ymm0,%xmm2",
                "sarl %r14d",
                "vpextrw $0,%xmm1,%edx",
                "andq $-32,%rsp",
                "sarl %r13d",
                ".cfi_offset3,-56",
                "vpextrw $1,%xmm1,%ebx",
                "vmovd %r9d,%xmm4",
                "movl %r10d,-4(%rsp)",
                "vpextrw $6,%xmm1,%r10d",
                "sarl %r12d",
                "movswl %r10w,%r10d",
                "vpextrw $2,%xmm1,%edi",
                "vmovd %esi,%xmm0",
                "movswl %dx,%edx",
                "sarl %r10d",
                "vpextrw $3,%xmm1,%r11d",
                "vpextrw $4,%xmm1,%ecx",
                "movswl %di,%edi",
                "movl %r10d,-8(%rsp)",
                "vmovd %r8d,%xmm3",
                "vpextrw $7,%xmm1,%r10d",
                "sarl %edx",
                "vmovd %eax,%xmm1",
                "vpinsrw $1,%r14d,%xmm4,%xmm4",
                "movswl %cx,%ecx",
                "movswl %bx,%ebx",
                "vpinsrw $1,%r12d,%xmm3,%xmm3",
                "sarl %edi",
                "movswl %r11w,%r11d",
                "movswl %r10w,%r10d",
                "sarl %ecx",
                "vpinsrw $1,%r15d,%xmm1,%xmm1",
                "vpinsrw $1,%r13d,%xmm0,%xmm0",
                "sarl %ebx",
                "vpunpckldq %xmm3,%xmm0,%xmm0",
                "vpunpckldq %xmm4,%xmm1,%xmm1",
                "sarl %r11d",
                "vmovd %edx,%xmm3",
                "vpunpcklqdq %xmm0,%xmm1,%xmm1",
                "sarl %r10d",
                "vmovd %edi,%xmm6",
                "vmovd %ecx,%xmm4",
                "vmovd -8(%rsp),%xmm0",
                "vpinsrw $1,-4(%rsp),%xmm4,%xmm4",
                "vpinsrw $1,%ebx,%xmm3,%xmm3",
                "vpinsrw $1,%r10d,%xmm0,%xmm0",
                "vpinsrw $1,%r11d,%xmm6,%xmm6",
                "leaq -40(%rbp),%rsp",
                "vpunpckldq %xmm0,%xmm4,%xmm4",
                "vpunpckldq %xmm6,%xmm3,%xmm3",
                "vpunpcklqdq %xmm4,%xmm3,%xmm3",
                "popq %rbx",
                "popq %r12",
                "vinsertf128 $0x1,%xmm3,%ymm1,%ymm1",
                "popq %r13",
                "popq %r14",
                "vmovdqa %xmm1,%xmm0",
                "vextractf128 $0x1,%ymm1,%xmm1",
                "popq %r15",
                "popq %rbp",
                ".cfi_def_cfa7,8",
                "vpaddw %xmm0,%xmm5,%xmm0",
                "vpaddw %xmm1,%xmm2,%xmm2",
                "vinsertf128 $0x1,%xmm2,%ymm0,%ymm0"
            ]
        },
        {
            "type": [
                "std::int8_t",
                "std::int8_t"
            ],
            "instr": [
                "pushq %rbp",
                ".cfi_def_cfa_offset16",
                ".cfi_offset6,-16",
                "vxorps %ymm1,%ymm0,%ymm2",
                "vandps %ymm1,%ymm0,%ymm0",
                "vpextrb $3,%xmm2,%edx",
                "vextractf128 $0x1,%ymm2,%xmm1",
                "vpextrb $0,%xmm2,%eax",
                "movsbl %dl,%edx",
                "vpextrb $2,%xmm2,%r9d",
                "vmovdqa %xmm0,%xmm4",
                "movsbl %al,%eax",
                "movq %rsp,%rbp",
                ".cfi_def_cfa_register6",
                "pushq %r15",
                "sarl %edx",
                ".cfi_offset15,-24",
                "vpextrb $1,%xmm2,%r15d",
                "pushq %r14",
                "movsbl %r9b,%r9d",
                "sarl %eax",
                "movsbl %r15b,%r15d",
                "pushq %r13",
                "sarl %r9d",
                "sarl %r15d",
                "vextractf128 $0x1,%ymm0,%xmm3",
                "pushq %r12",
                ".cfi_offset14,-32",
                ".cfi_offset13,-40",
                ".cfi_offset12,-48",
                "vpextrb $5,%xmm1,%r14d",
                "vmovd %eax,%xmm6",
                "vpextrb $6,%xmm1,%r8d",
                "pushq %rbx",
                "vpextrb $7,%xmm1,%r13d",
                "vmovd %r9d,%xmm9",
                "vpextrb $9,%xmm1,%r12d",
                "vpextrb $10,%xmm1,%edi",
                ".cfi_offset3,-56",
                "vpextrb $11,%xmm1,%ebx",
                "movsbl %r8b,%r8d",
                "movsbl %r14b,%r14d",
                "vpextrb $12,%xmm1,%ecx",
                "vpextrb $13,%xmm1,%r11d",
                "movsbl %dil,%edi",
                "sarl %r8d",
                "andq $-32,%rsp",
                "vpextrb $14,%xmm1,%esi",
                "movsbl %cl,%ecx",
                "sarl %edi",
                "movl %edx,-4(%rsp)",
                "vpextrb $4,%xmm2,%edx",
                "movsbl %sil,%esi",
                "movsbl %dl,%edx",
                "vpextrb $15,%xmm1,%r10d",
                "sarl %ecx",
                "movsbl %r13b,%r13d",
                "sarl %edx",
                "vpinsrb $1,-4(%rsp),%xmm9,%xmm9",
                "sarl %esi",
                "movsbl %r12b,%r12d",
                "movl %edx,-8(%rsp)",
                "vpextrb $5,%xmm2,%edx",
                "movsbl %bl,%ebx",
                "sarl %r14d",
                "movsbl %dl,%edx",
                "vpinsrb $1,%r15d,%xmm6,%xmm6",
                "movsbl %r11b,%r11d",
                "sarl %r13d",
                "sarl %edx",
                "vpunpcklwd %xmm9,%xmm6,%xmm6",
                "movsbl %r10b,%r10d",
                "sarl %r12d",
                "movl %edx,-12(%rsp)",
                "vpextrb $6,%xmm2,%edx",
                "sarl %ebx",
                "vmovd %r8d,%xmm9",
                "movsbl %dl,%edx",
                "sarl %r11d",
                "vpinsrb $1,%r13d,%xmm9,%xmm9",
                "sarl %edx",
                "movl %edx,-16(%rsp)",
                "vpextrb $7,%xmm2,%edx",
                "movsbl %dl,%edx",
                "sarl %edx",
                "movl %edx,-20(%rsp)",
                "vpextrb $8,%xmm2,%edx",
                "vmovd -16(%rsp),%xmm8",
                "vpinsrb $1,-20(%rsp),%xmm8,%xmm8",
                "movsbl %dl,%edx",
                "sarl %edx",
                "movl %edx,-24(%rsp)",
                "vpextrb $9,%xmm2,%edx",
                "movsbl %dl,%edx",
                "sarl %edx",
                "movl %edx,-28(%rsp)",
                "vpextrb $10,%xmm2,%edx",
                "movsbl %dl,%edx",
                "sarl %edx",
                "movl %edx,-32(%rsp)",
                "vpextrb $11,%xmm2,%edx",
                "movsbl %dl,%edx",
                "sarl %edx",
                "movl %edx,-36(%rsp)",
                "vpextrb $12,%xmm2,%edx",
                "movsbl %dl,%edx",
                "sarl %edx",
                "movl %edx,-40(%rsp)",
                "vpextrb $13,%xmm2,%edx",
                "movsbl %dl,%edx",
                "sarl %edx",
                "movl %edx,-44(%rsp)",
                "vpextrb $14,%xmm2,%edx",
                "movsbl %dl,%edx",
                "sarl %edx",
                "movl %edx,-48(%rsp)",
                "vpextrb $15,%xmm2,%edx",
                "vmovd -8(%rsp),%xmm2",
                "vpinsrb $1,-12(%rsp),%xmm2,%xmm2",
                "movsbl %dl,%edx",
                "sarl %edx",
                "vpunpcklwd %xmm8,%xmm2,%xmm2",
                "vmovd %edi,%xmm8",
                "movl %edx,-52(%rsp)",
                "vpextrb $0,%xmm1,%edx",
                "vpunpckldq %xmm2,%xmm6,%xmm6",
                "vpinsrb $1,%ebx,%xmm8,%xmm8",
                "movsbl %dl,%edx",
                "sarl %edx",
                "movl %edx,-56(%rsp)",
                "vpextrb $1,%xmm1,%edx",
                "movsbl %dl,%edx",
                "sarl %edx",
                "movl %edx,-60(%rsp)",
                "vpextrb $2,%xmm1,%edx",
                "movsbl %dl,%edx",
                "sarl %edx",
                "movl %edx,-64(%rsp)",
                "vpextrb $3,%xmm1,%edx",
                "movsbl %dl,%edx",
                "sarl %edx",
                "movl %edx,-68(%rsp)",
                "vpextrb $4,%xmm1,%edx",
                "movsbl %dl,%edx",
                "sarl %edx",
                "movl %edx,-72(%rsp)",
                "vpextrb $8,%xmm1,%edx",
                "vmovd -24(%rsp),%xmm1",
                "vpinsrb $1,-28(%rsp),%xmm1,%xmm1",
                "vmovd -32(%rsp),%xmm7",
                "vmovd -40(%rsp),%xmm0",
                "vmovd -48(%rsp),%xmm5",
                "movsbl %dl,%edx",
                "vpinsrb $1,-36(%rsp),%xmm7,%xmm7",
                "sarl %edx",
                "vmovd -56(%rsp),%xmm2",
                "vmovd -64(%rsp),%xmm10",
                "vpinsrb $1,-52(%rsp),%xmm5,%xmm5",
                "vpinsrb $1,-44(%rsp),%xmm0,%xmm0",
                "sarl %r10d",
                "vpunpcklwd %xmm7,%xmm1,%xmm1",
                "vpinsrb $1,-60(%rsp),%xmm2,%xmm2",
                "vmovd %esi,%xmm7",
                "vpunpcklwd %xmm5,%xmm0,%xmm0",
                "vpinsrb $1,-68(%rsp),%xmm10,%xmm10",
                "vmovd -72(%rsp),%xmm5",
                "vpunpckldq %xmm0,%xmm1,%xmm1",
                "vmovd %ecx,%xmm0",
                "vpinsrb $1,%r14d,%xmm5,%xmm5",
                "vpunpcklqdq %xmm1,%xmm6,%xmm6",
                "vmovd %edx,%xmm1",
                "vpinsrb $1,%r11d,%xmm0,%xmm0",
                "vpinsrb $1,%r12d,%xmm1,%xmm1",
                "vpinsrb $1,%r10d,%xmm7,%xmm7",
                "vpunpcklwd %xmm10,%xmm2,%xmm2",
                "vpunpcklwd %xmm7,%xmm0,%xmm0",
                "vpunpcklwd %xmm9,%xmm5,%xmm5",
                "vpunpcklwd %xmm8,%xmm1,%xmm1",
                "vpunpckldq %xmm0,%xmm1,%xmm1",
                "vpunpckldq %xmm5,%xmm2,%xmm2",
                "leaq -40(%rbp),%rsp",
                "vpunpcklqdq %xmm1,%xmm2,%xmm1",
                "popq %rbx",
                "popq %r12",
                "vinsertf128 $0x1,%xmm1,%ymm6,%ymm1",
                "popq %r13",
                "popq %r14",
                "vmovdqa %xmm1,%xmm0",
                "vextractf128 $0x1,%ymm1,%xmm1",
                "popq %r15",
                "popq %rbp",
                ".cfi_def_cfa7,8",
                "vpaddb %xmm0,%xmm4,%xmm0",
                "vpaddb %xmm1,%xmm3,%xmm3",
                "vinsertf128 $0x1,%xmm3,%ymm0,%ymm0"
            ]
        },
        {
            "type": [
                "std::uint64_t",
                "std::uint64_t"
            ],
            "instr": [
                "vxorps %ymm1,%ymm0,%ymm2",
                "vandps %ymm1,%ymm0,%ymm0",
                "vextractf128 $0x1,%ymm2,%xmm1",
                "vmovq %xmm2,%rax",
                "vpextrq $1,%xmm2,%rcx",
                "shrq %rax",
                "vmovq %xmm1,%rdx",
                "shrq %rcx",
                "vmovdqa %xmm0,%xmm3",
                "shrq %rdx",
                "vmovq %rax,%xmm4",
                "vextractf128 $0x1,%ymm0,%xmm0",
                "vmovq %rdx,%xmm2",
                "vpextrq $1,%xmm1,%rdx",
                "vpinsrq $1,%rcx,%xmm4,%xmm1",
                "shrq %rdx",
                "vpinsrq $1,%rdx,%xmm2,%xmm2",
                "vinsertf128 $0x1,%xmm2,%ymm1,%ymm1",
                "vmovdqa %xmm1,%xmm2",
                "vextractf128 $0x1,%ymm1,%xmm1",
                "vpaddq %xmm2,%xmm3,%xmm2",
                "vpaddq %xmm1,%xmm0,%xmm0",
                "vinsertf128 $0x1,%xmm0,%ymm2,%ymm0"
            ]
        },
        {
            "type": [
                "std::uint32_t",
                "std::uint32_t"
            ],
            "instr": [
                "vxorps %ymm1,%ymm0,%ymm2",
                "vmovdqa %ymm1,%ymm6",
                "vmovd %xmm2,%eax",
                "vpextrd $1,%xmm2,%esi",
                "vpextrd $3,%xmm2,%edx",
                "shrl %eax",
                "shrl %esi",
                "vandps %ymm6,%ymm0,%ymm0",
                "movl %eax,%ecx",
                "vpextrd $2,%xmm2,%eax",
                "vextractf128 $0x1,%ymm2,%xmm2",
                "shrl %edx",
                "vpextrd $1,%xmm2,%edi",
                "shrl %eax",
                "vmovd %ecx,%xmm7",
                "vmovdqa %xmm0,%xmm3",
                "vmovd %eax,%xmm5",
                "vmovd %xmm2,%eax",
                "shrl %edi",
                "vextractf128 $0x1,%ymm0,%xmm0",
                "shrl %eax",
                "vpinsrd $1,%edx,%xmm5,%xmm5",
                "vmovd %eax,%xmm1",
                "vpextrd $2,%xmm2,%eax",
                "shrl %eax",
                "vpinsrd $1,%edi,%xmm1,%xmm1",
                "vmovd %eax,%xmm4",
                "vpextrd $3,%xmm2,%eax",
                "vpinsrd $1,%esi,%xmm7,%xmm2",
                "shrl %eax",
                "vpunpcklqdq %xmm5,%xmm2,%xmm2",
                "vpinsrd $1,%eax,%xmm4,%xmm4",
                "vpaddd %xmm2,%xmm3,%xmm2",
                "vpunpcklqdq %xmm4,%xmm1,%xmm1",
                "vpaddd %xmm1,%xmm0,%xmm0",
                "vinsertf128 $0x1,%xmm0,%ymm2,%ymm0"
            ]
        },
        {
            "type": [
                "std::uint16_t",
                "std::uint16_t"
            ],
            "instr": [
                "pushq %rbp",
                ".cfi_def_cfa_offset16",
                ".cfi_offset6,-16",
                "vxorps %ymm1,%ymm0,%ymm3",
                "vandps %ymm1,%ymm0,%ymm0",
                "vextractf128 $0x1,%ymm3,%xmm1",
                "vpextrw $0,%xmm3,%eax",
                "vpextrw $2,%xmm3,%r9d",
                "vpextrw $5,%xmm1,%r10d",
                "vpextrw $4,%xmm3,%esi",
                "sarl %eax",
                "vmovdqa %xmm0,%xmm5",
                "movq %rsp,%rbp",
                ".cfi_def_cfa_register6",
                "pushq %r15",
                "sarl %r10d",
                "vpextrw $6,%xmm3,%r8d",
                "pushq %r14",
                ".cfi_offset15,-24",
                ".cfi_offset14,-32",
                "vpextrw $1,%xmm3,%r15d",
                "vpextrw $3,%xmm3,%r14d",
                "sarl %r9d",
                "pushq %r13",
                "sarl %esi",
                ".cfi_offset13,-40",
                "vpextrw $5,%xmm3,%r13d",
                "sarl %r8d",
                "pushq %r12",
                ".cfi_offset12,-48",
                "vpextrw $7,%xmm3,%r12d",
                "vextractf128 $0x1,%ymm0,%xmm2",
                "sarl %r15d",
                "pushq %rbx",
                "sarl %r14d",
                "sarl %r13d",
                "vpextrw $0,%xmm1,%edx",
                "sarl %r12d",
                ".cfi_offset3,-56",
                "vpextrw $1,%xmm1,%ebx",
                "vpextrw $2,%xmm1,%edi",
                "sarl %edx",
                "vpextrw $3,%xmm1,%r11d",
                "vpextrw $4,%xmm1,%ecx",
                "vmovd %r9d,%xmm4",
                "sarl %edi",
                "andq $-32,%rsp",
                "vmovd %esi,%xmm0",
                "vmovd %r8d,%xmm3",
                "movl %r10d,-4(%rsp)",
                "vpextrw $6,%xmm1,%r10d",
                "sarl %ecx",
                "vmovd %edi,%xmm6",
                "sarl %r10d",
                "vpinsrw $1,%r14d,%xmm4,%xmm4",
                "vpinsrw $1,%r12d,%xmm3,%xmm3",
                "sarl %ebx",
                "movl %r10d,-8(%rsp)",
                "vpextrw $7,%xmm1,%r10d",
                "vmovd %eax,%xmm1",
                "sarl %r11d",
                "vpinsrw $1,%r15d,%xmm1,%xmm1",
                "vpinsrw $1,%r13d,%xmm0,%xmm0",
                "sarl %r10d",
                "vpinsrw $1,%r11d,%xmm6,%xmm6",
                "vpunpckldq %xmm3,%xmm0,%xmm0",
                "vpunpckldq %xmm4,%xmm1,%xmm1",
                "vmovd %edx,%xmm3",
                "vpunpcklqdq %xmm0,%xmm1,%xmm1",
                "vmovd %ecx,%xmm4",
                "vmovd -8(%rsp),%xmm0",
                "vpinsrw $1,-4(%rsp),%xmm4,%xmm4",
                "vpinsrw $1,%ebx,%xmm3,%xmm3",
                "vpinsrw $1,%r10d,%xmm0,%xmm0",
                "vpunpckldq %xmm6,%xmm3,%xmm3",
                "leaq -40(%rbp),%rsp",
                "vpunpckldq %xmm0,%xmm4,%xmm4",
                "vpunpcklqdq %xmm4,%xmm3,%xmm3",
                "popq %rbx",
                "popq %r12",
                "vinsertf128 $0x1,%xmm3,%ymm1,%ymm1",
                "popq %r13",
                "popq %r14",
                "vmovdqa %xmm1,%xmm0",
                "vextractf128 $0x1,%ymm1,%xmm1",
                "popq %r15",
                "popq %rbp",
                ".cfi_def_cfa7,8",
                "vpaddw %xmm0,%xmm5,%xmm0",
                "vpaddw %xmm1,%xmm2,%xmm2",
                "vinsertf128 $0x1,%xmm2,%ymm0,%ymm0"
            ]
        },
        {
            "type": [
                "std::uint8_t",
                "std::uint8_t"
            ],
            "instr": [
                "pushq %rbp",
                ".cfi_def_cfa_offset16",
                ".cfi_offset6,-16",
                "vxorps %ymm1,%ymm0,%ymm2",
                "vandps %ymm1,%ymm0,%ymm0",
                "vpextrb $6,%xmm2,%edx",
                "vextractf128 $0x1,%ymm2,%xmm1",
                "vmovdqa %xmm0,%xmm4",
                "sarl %edx",
                "vpextrb $0,%xmm2,%eax",
                "vextractf128 $0x1,%ymm0,%xmm3",
                "movq %rsp,%rbp",
                ".cfi_def_cfa_register6",
                "pushq %r15",
                "vpextrb $2,%xmm2,%r9d",
                "vpextrb $4,%xmm2,%esi",
                "pushq %r14",
                "sarl %eax",
                ".cfi_offset15,-24",
                ".cfi_offset14,-32",
                "vpextrb $1,%xmm2,%r15d",
                "sarl %r9d",
                "pushq %r13",
                "vpextrb $3,%xmm2,%r14d",
                "sarl %esi",
                ".cfi_offset13,-40",
                "vpextrb $5,%xmm2,%r13d",
                "pushq %r12",
                "sarl %r15d",
                "sarl %r14d",
                ".cfi_offset12,-48",
                "vpextrb $9,%xmm1,%r12d",
                "pushq %rbx",
                "sarl %r13d",
                "vpextrb $10,%xmm1,%r8d",
                ".cfi_offset3,-56",
                "vpextrb $11,%xmm1,%ebx",
                "vpextrb $12,%xmm1,%ecx",
                "vmovd %eax,%xmm6",
                "vmovd %r9d,%xmm9",
                "sarl %ebx",
                "vpextrb $13,%xmm1,%r11d",
                "vpextrb $14,%xmm1,%edi",
                "sarl %r8d",
                "vpextrb $15,%xmm1,%r10d",
                "andq $-32,%rsp",
                "vpinsrb $1,%r14d,%xmm9,%xmm9",
                "sarl %ecx",
                "movl %edx,-4(%rsp)",
                "sarl %edi",
                "vpextrb $7,%xmm2,%edx",
                "sarl %r12d",
                "sarl %edx",
                "vpinsrb $1,%r15d,%xmm6,%xmm6",
                "sarl %r11d",
                "movl %edx,-8(%rsp)",
                "vpextrb $8,%xmm2,%edx",
                "vpunpcklwd %xmm9,%xmm6,%xmm6",
                "sarl %edx",
                "movl %edx,-12(%rsp)",
                "vpextrb $9,%xmm2,%edx",
                "sarl %edx",
                "vmovd -4(%rsp),%xmm8",
                "vpinsrb $1,-8(%rsp),%xmm8,%xmm8",
                "movl %edx,-16(%rsp)",
                "vpextrb $10,%xmm2,%edx",
                "sarl %edx",
                "movl %edx,-20(%rsp)",
                "vpextrb $11,%xmm2,%edx",
                "sarl %edx",
                "movl %edx,-24(%rsp)",
                "vpextrb $12,%xmm2,%edx",
                "sarl %edx",
                "vmovd -20(%rsp),%xmm7",
                "vpinsrb $1,-24(%rsp),%xmm7,%xmm7",
                "movl %edx,-28(%rsp)",
                "vpextrb $13,%xmm2,%edx",
                "sarl %edx",
                "movl %edx,-32(%rsp)",
                "vpextrb $14,%xmm2,%edx",
                "sarl %edx",
                "movl %edx,-36(%rsp)",
                "vpextrb $15,%xmm2,%edx",
                "vmovd %esi,%xmm2",
                "sarl %edx",
                "vpinsrb $1,%r13d,%xmm2,%xmm2",
                "movl %edx,-40(%rsp)",
                "vpextrb $0,%xmm1,%edx",
                "vpunpcklwd %xmm8,%xmm2,%xmm2",
                "vmovd %r8d,%xmm8",
                "sarl %edx",
                "vpunpckldq %xmm2,%xmm6,%xmm6",
                "vpinsrb $1,%ebx,%xmm8,%xmm8",
                "movl %edx,-44(%rsp)",
                "vpextrb $1,%xmm1,%edx",
                "sarl %edx",
                "movl %edx,-48(%rsp)",
                "vpextrb $2,%xmm1,%edx",
                "sarl %edx",
                "movl %edx,-52(%rsp)",
                "vpextrb $3,%xmm1,%edx",
                "sarl %edx",
                "movl %edx,-56(%rsp)",
                "vpextrb $4,%xmm1,%edx",
                "sarl %edx",
                "movl %edx,-60(%rsp)",
                "vpextrb $5,%xmm1,%edx",
                "sarl %edx",
                "movl %edx,-64(%rsp)",
                "vpextrb $6,%xmm1,%edx",
                "sarl %edx",
                "movl %edx,-68(%rsp)",
                "vpextrb $7,%xmm1,%edx",
                "sarl %edx",
                "movl %edx,-72(%rsp)",
                "vpextrb $8,%xmm1,%edx",
                "vmovd -12(%rsp),%xmm1",
                "vpinsrb $1,-16(%rsp),%xmm1,%xmm1",
                "vmovd -28(%rsp),%xmm0",
                "vmovd -36(%rsp),%xmm5",
                "sarl %edx",
                "vpinsrb $1,-40(%rsp),%xmm5,%xmm5",
                "vpinsrb $1,-32(%rsp),%xmm0,%xmm0",
                "vpunpcklwd %xmm7,%xmm1,%xmm1",
                "sarl %r10d",
                "vmovd -44(%rsp),%xmm2",
                "vmovd -52(%rsp),%xmm10",
                "vpinsrb $1,-48(%rsp),%xmm2,%xmm2",
                "vpinsrb $1,-56(%rsp),%xmm10,%xmm10",
                "vpunpcklwd %xmm5,%xmm0,%xmm0",
                "vmovd -68(%rsp),%xmm9",
                "vmovd -60(%rsp),%xmm5",
                "vpunpckldq %xmm0,%xmm1,%xmm1",
                "vmovd %edi,%xmm7",
                "vmovd %ecx,%xmm0",
                "vpinsrb $1,-64(%rsp),%xmm5,%xmm5",
                "vpunpcklqdq %xmm1,%xmm6,%xmm6",
                "vmovd %edx,%xmm1",
                "vpinsrb $1,%r12d,%xmm1,%xmm1",
                "vpinsrb $1,%r11d,%xmm0,%xmm0",
                "vpinsrb $1,%r10d,%xmm7,%xmm7",
                "vpinsrb $1,-72(%rsp),%xmm9,%xmm9",
                "vpunpcklwd %xmm7,%xmm0,%xmm0",
                "vpunpcklwd %xmm10,%xmm2,%xmm2",
                "vpunpcklwd %xmm8,%xmm1,%xmm1",
                "leaq -40(%rbp),%rsp",
                "vpunpcklwd %xmm9,%xmm5,%xmm5",
                "vpunpckldq %xmm0,%xmm1,%xmm1",
                "vpunpckldq %xmm5,%xmm2,%xmm2",
                "popq %rbx",
                "popq %r12",
                "vpunpcklqdq %xmm1,%xmm2,%xmm1",
                "popq %r13",
                "popq %r14",
                "vinsertf128 $0x1,%xmm1,%ymm6,%ymm1",
                "popq %r15",
                "popq %rbp",
                ".cfi_def_cfa7,8",
                "vmovdqa %xmm1,%xmm0",
                "vextractf128 $0x1,%ymm1,%xmm1",
                "vpaddb %xmm0,%xmm4,%xmm0",
                "vpaddb %xmm1,%xmm3,%xmm3",
                "vinsertf128 $0x1,%xmm3,%ymm0,%ymm0"
            ]
        },
        {
            "type": [
                "std::uint64_t",
                "std::uint64_t"
            ],
            "instr": [
                "vxorps %ymm1,%ymm0,%ymm2",
                "vandps %ymm1,%ymm0,%ymm0",
                "vextractf128 $0x1,%ymm2,%xmm1",
                "vmovq %xmm2,%rax",
                "vpextrq $1,%xmm2,%rcx",
                "shrq %rax",
                "vmovq %xmm1,%rdx",
                "shrq %rcx",
                "vmovdqa %xmm0,%xmm3",
                "shrq %rdx",
                "vmovq %rax,%xmm4",
                "vextractf128 $0x1,%ymm0,%xmm0",
                "vmovq %rdx,%xmm2",
                "vpextrq $1,%xmm1,%rdx",
                "vpinsrq $1,%rcx,%xmm4,%xmm1",
                "shrq %rdx",
                "vpinsrq $1,%rdx,%xmm2,%xmm2",
                "vinsertf128 $0x1,%xmm2,%ymm1,%ymm1",
                "vmovdqa %xmm1,%xmm2",
                "vextractf128 $0x1,%ymm1,%xmm1",
                "vpaddq %xmm2,%xmm3,%xmm2",
                "vpaddq %xmm1,%xmm0,%xmm0",
                "vinsertf128 $0x1,%xmm0,%ymm2,%ymm0"
            ]
        },
        {
            "type": [
                "std::uint32_t",
                "std::uint32_t"
            ],
            "instr": [
                "vxorps %ymm1,%ymm0,%ymm2",
                "vmovdqa %ymm1,%ymm6",
                "vmovd %xmm2,%eax",
                "vpextrd $1,%xmm2,%esi",
                "vpextrd $3,%xmm2,%edx",
                "shrl %eax",
                "shrl %esi",
                "vandps %ymm6,%ymm0,%ymm0",
                "movl %eax,%ecx",
                "vpextrd $2,%xmm2,%eax",
                "vextractf128 $0x1,%ymm2,%xmm2",
                "shrl %edx",
                "vpextrd $1,%xmm2,%edi",
                "shrl %eax",
                "vmovd %ecx,%xmm7",
                "vmovdqa %xmm0,%xmm3",
                "vmovd %eax,%xmm5",
                "vmovd %xmm2,%eax",
                "shrl %edi",
                "vextractf128 $0x1,%ymm0,%xmm0",
                "shrl %eax",
                "vpinsrd $1,%edx,%xmm5,%xmm5",
                "vmovd %eax,%xmm1",
                "vpextrd $2,%xmm2,%eax",
                "shrl %eax",
                "vpinsrd $1,%edi,%xmm1,%xmm1",
                "vmovd %eax,%xmm4",
                "vpextrd $3,%xmm2,%eax",
                "vpinsrd $1,%esi,%xmm7,%xmm2",
                "shrl %eax",
                "vpunpcklqdq %xmm5,%xmm2,%xmm2",
                "vpinsrd $1,%eax,%xmm4,%xmm4",
                "vpaddd %xmm2,%xmm3,%xmm2",
                "vpunpcklqdq %xmm4,%xmm1,%xmm1",
                "vpaddd %xmm1,%xmm0,%xmm0",
                "vinsertf128 $0x1,%xmm0,%ymm2,%ymm0"
            ]
        },
        {
            "type": [
                "std::uint16_t",
                "std::uint16_t"
            ],
            "instr": [
                "pushq %rbp",
                ".cfi_def_cfa_offset16",
                ".cfi_offset6,-16",
                "vxorps %ymm1,%ymm0,%ymm3",
                "vandps %ymm1,%ymm0,%ymm0",
                "vextractf128 $0x1,%ymm3,%xmm1",
                "vpextrw $0,%xmm3,%eax",
                "vpextrw $2,%xmm3,%r9d",
                "vpextrw $5,%xmm1,%r10d",
                "vpextrw $4,%xmm3,%esi",
                "sarl %eax",
                "vmovdqa %xmm0,%xmm5",
                "movq %rsp,%rbp",
                ".cfi_def_cfa_register6",
                "pushq %r15",
                "sarl %r10d",
                "vpextrw $6,%xmm3,%r8d",
                "pushq %r14",
                ".cfi_offset15,-24",
                ".cfi_offset14,-32",
                "vpextrw $1,%xmm3,%r15d",
                "vpextrw $3,%xmm3,%r14d",
                "sarl %r9d",
                "pushq %r13",
                "sarl %esi",
                ".cfi_offset13,-40",
                "vpextrw $5,%xmm3,%r13d",
                "sarl %r8d",
                "pushq %r12",
                ".cfi_offset12,-48",
                "vpextrw $7,%xmm3,%r12d",
                "vextractf128 $0x1,%ymm0,%xmm2",
                "sarl %r15d",
                "pushq %rbx",
                "sarl %r14d",
                "sarl %r13d",
                "vpextrw $0,%xmm1,%edx",
                "sarl %r12d",
                ".cfi_offset3,-56",
                "vpextrw $1,%xmm1,%ebx",
                "vpextrw $2,%xmm1,%edi",
                "sarl %edx",
                "vpextrw $3,%xmm1,%r11d",
                "vpextrw $4,%xmm1,%ecx",
                "vmovd %r9d,%xmm4",
                "sarl %edi",
                "andq $-32,%rsp",
                "vmovd %esi,%xmm0",
                "vmovd %r8d,%xmm3",
                "movl %r10d,-4(%rsp)",
                "vpextrw $6,%xmm1,%r10d",
                "sarl %ecx",
                "vmovd %edi,%xmm6",
                "sarl %r10d",
                "vpinsrw $1,%r14d,%xmm4,%xmm4",
                "vpinsrw $1,%r12d,%xmm3,%xmm3",
                "sarl %ebx",
                "movl %r10d,-8(%rsp)",
                "vpextrw $7,%xmm1,%r10d",
                "vmovd %eax,%xmm1",
                "sarl %r11d",
                "vpinsrw $1,%r15d,%xmm1,%xmm1",
                "vpinsrw $1,%r13d,%xmm0,%xmm0",
                "sarl %r10d",
                "vpinsrw $1,%r11d,%xmm6,%xmm6",
                "vpunpckldq %xmm3,%xmm0,%xmm0",
                "vpunpckldq %xmm4,%xmm1,%xmm1",
                "vmovd %edx,%xmm3",
                "vpunpcklqdq %xmm0,%xmm1,%xmm1",
                "vmovd %ecx,%xmm4",
                "vmovd -8(%rsp),%xmm0",
                "vpinsrw $1,-4(%rsp),%xmm4,%xmm4",
                "vpinsrw $1,%ebx,%xmm3,%xmm3",
                "vpinsrw $1,%r10d,%xmm0,%xmm0",
                "vpunpckldq %xmm6,%xmm3,%xmm3",
                "leaq -40(%rbp),%rsp",
                "vpunpckldq %xmm0,%xmm4,%xmm4",
                "vpunpcklqdq %xmm4,%xmm3,%xmm3",
                "popq %rbx",
                "popq %r12",
                "vinsertf128 $0x1,%xmm3,%ymm1,%ymm1",
                "popq %r13",
                "popq %r14",
                "vmovdqa %xmm1,%xmm0",
                "vextractf128 $0x1,%ymm1,%xmm1",
                "popq %r15",
                "popq %rbp",
                ".cfi_def_cfa7,8",
                "vpaddw %xmm0,%xmm5,%xmm0",
                "vpaddw %xmm1,%xmm2,%xmm2",
                "vinsertf128 $0x1,%xmm2,%ymm0,%ymm0"
            ]
        },
        {
            "type": [
                "std::uint8_t",
                "std::uint8_t"
            ],
            "instr": [
                "pushq %rbp",
                ".cfi_def_cfa_offset16",
                ".cfi_offset6,-16",
                "vxorps %ymm1,%ymm0,%ymm2",
                "vandps %ymm1,%ymm0,%ymm0",
                "vpextrb $6,%xmm2,%edx",
                "vextractf128 $0x1,%ymm2,%xmm1",
                "vmovdqa %xmm0,%xmm4",
                "sarl %edx",
                "vpextrb $0,%xmm2,%eax",
                "vextractf128 $0x1,%ymm0,%xmm3",
                "movq %rsp,%rbp",
                ".cfi_def_cfa_register6",
                "pushq %r15",
                "vpextrb $2,%xmm2,%r9d",
                "vpextrb $4,%xmm2,%esi",
                "pushq %r14",
                "sarl %eax",
                ".cfi_offset15,-24",
                ".cfi_offset14,-32",
                "vpextrb $1,%xmm2,%r15d",
                "sarl %r9d",
                "pushq %r13",
                "vpextrb $3,%xmm2,%r14d",
                "sarl %esi",
                ".cfi_offset13,-40",
                "vpextrb $5,%xmm2,%r13d",
                "pushq %r12",
                "sarl %r15d",
                "sarl %r14d",
                ".cfi_offset12,-48",
                "vpextrb $9,%xmm1,%r12d",
                "pushq %rbx",
                "sarl %r13d",
                "vpextrb $10,%xmm1,%r8d",
                ".cfi_offset3,-56",
                "vpextrb $11,%xmm1,%ebx",
                "vpextrb $12,%xmm1,%ecx",
                "vmovd %eax,%xmm6",
                "vmovd %r9d,%xmm9",
                "sarl %ebx",
                "vpextrb $13,%xmm1,%r11d",
                "vpextrb $14,%xmm1,%edi",
                "sarl %r8d",
                "vpextrb $15,%xmm1,%r10d",
                "andq $-32,%rsp",
                "vpinsrb $1,%r14d,%xmm9,%xmm9",
                "sarl %ecx",
                "movl %edx,-4(%rsp)",
                "sarl %edi",
                "vpextrb $7,%xmm2,%edx",
                "sarl %r12d",
                "sarl %edx",
                "vpinsrb $1,%r15d,%xmm6,%xmm6",
                "sarl %r11d",
                "movl %edx,-8(%rsp)",
                "vpextrb $8,%xmm2,%edx",
                "vpunpcklwd %xmm9,%xmm6,%xmm6",
                "sarl %edx",
                "movl %edx,-12(%rsp)",
                "vpextrb $9,%xmm2,%edx",
                "sarl %edx",
                "vmovd -4(%rsp),%xmm8",
                "vpinsrb $1,-8(%rsp),%xmm8,%xmm8",
                "movl %edx,-16(%rsp)",
                "vpextrb $10,%xmm2,%edx",
                "sarl %edx",
                "movl %edx,-20(%rsp)",
                "vpextrb $11,%xmm2,%edx",
                "sarl %edx",
                "movl %edx,-24(%rsp)",
                "vpextrb $12,%xmm2,%edx",
                "sarl %edx",
                "vmovd -20(%rsp),%xmm7",
                "vpinsrb $1,-24(%rsp),%xmm7,%xmm7",
                "movl %edx,-28(%rsp)",
                "vpextrb $13,%xmm2,%edx",
                "sarl %edx",
                "movl %edx,-32(%rsp)",
                "vpextrb $14,%xmm2,%edx",
                "sarl %edx",
                "movl %edx,-36(%rsp)",
                "vpextrb $15,%xmm2,%edx",
                "vmovd %esi,%xmm2",
                "sarl %edx",
                "vpinsrb $1,%r13d,%xmm2,%xmm2",
                "movl %edx,-40(%rsp)",
                "vpextrb $0,%xmm1,%edx",
                "vpunpcklwd %xmm8,%xmm2,%xmm2",
                "vmovd %r8d,%xmm8",
                "sarl %edx",
                "vpunpckldq %xmm2,%xmm6,%xmm6",
                "vpinsrb $1,%ebx,%xmm8,%xmm8",
                "movl %edx,-44(%rsp)",
                "vpextrb $1,%xmm1,%edx",
                "sarl %edx",
                "movl %edx,-48(%rsp)",
                "vpextrb $2,%xmm1,%edx",
                "sarl %edx",
                "movl %edx,-52(%rsp)",
                "vpextrb $3,%xmm1,%edx",
                "sarl %edx",
                "movl %edx,-56(%rsp)",
                "vpextrb $4,%xmm1,%edx",
                "sarl %edx",
                "movl %edx,-60(%rsp)",
                "vpextrb $5,%xmm1,%edx",
                "sarl %edx",
                "movl %edx,-64(%rsp)",
                "vpextrb $6,%xmm1,%edx",
                "sarl %edx",
                "movl %edx,-68(%rsp)",
                "vpextrb $7,%xmm1,%edx",
                "sarl %edx",
                "movl %edx,-72(%rsp)",
                "vpextrb $8,%xmm1,%edx",
                "vmovd -12(%rsp),%xmm1",
                "vpinsrb $1,-16(%rsp),%xmm1,%xmm1",
                "vmovd -28(%rsp),%xmm0",
                "vmovd -36(%rsp),%xmm5",
                "sarl %edx",
                "vpinsrb $1,-40(%rsp),%xmm5,%xmm5",
                "vpinsrb $1,-32(%rsp),%xmm0,%xmm0",
                "vpunpcklwd %xmm7,%xmm1,%xmm1",
                "sarl %r10d",
                "vmovd -44(%rsp),%xmm2",
                "vmovd -52(%rsp),%xmm10",
                "vpinsrb $1,-48(%rsp),%xmm2,%xmm2",
                "vpinsrb $1,-56(%rsp),%xmm10,%xmm10",
                "vpunpcklwd %xmm5,%xmm0,%xmm0",
                "vmovd -68(%rsp),%xmm9",
                "vmovd -60(%rsp),%xmm5",
                "vpunpckldq %xmm0,%xmm1,%xmm1",
                "vmovd %edi,%xmm7",
                "vmovd %ecx,%xmm0",
                "vpinsrb $1,-64(%rsp),%xmm5,%xmm5",
                "vpunpcklqdq %xmm1,%xmm6,%xmm6",
                "vmovd %edx,%xmm1",
                "vpinsrb $1,%r12d,%xmm1,%xmm1",
                "vpinsrb $1,%r11d,%xmm0,%xmm0",
                "vpinsrb $1,%r10d,%xmm7,%xmm7",
                "vpinsrb $1,-72(%rsp),%xmm9,%xmm9",
                "vpunpcklwd %xmm7,%xmm0,%xmm0",
                "vpunpcklwd %xmm10,%xmm2,%xmm2",
                "vpunpcklwd %xmm8,%xmm1,%xmm1",
                "leaq -40(%rbp),%rsp",
                "vpunpcklwd %xmm9,%xmm5,%xmm5",
                "vpunpckldq %xmm0,%xmm1,%xmm1",
                "vpunpckldq %xmm5,%xmm2,%xmm2",
                "popq %rbx",
                "popq %r12",
                "vpunpcklqdq %xmm1,%xmm2,%xmm1",
                "popq %r13",
                "popq %r14",
                "vinsertf128 $0x1,%xmm1,%ymm6,%ymm1",
                "popq %r15",
                "popq %rbp",
                ".cfi_def_cfa7,8",
                "vmovdqa %xmm1,%xmm0",
                "vextractf128 $0x1,%ymm1,%xmm1",
                "vpaddb %xmm0,%xmm4,%xmm0",
                "vpaddb %xmm1,%xmm3,%xmm3",
                "vinsertf128 $0x1,%xmm3,%ymm0,%ymm0"
            ]
        }
    ]
}